{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3910, 3910)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1075/1075 [1:04:29<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 finished\n",
      "(3643, 3643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1075/1075 [53:41<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 finished\n",
      "(3201, 3201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 636/1075 [23:23<15:46,  2.16s/it]"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#\n",
    "#不使用impute\n",
    "#####依赖higashi使用原始的稀疏矩阵计算sc和prebulk的pc值\n",
    "##########################################################\n",
    "#############################################################\n",
    "#############################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "from Higashi_backend.utils import *\n",
    "from Higashi_analysis.Higashi_analysis import *\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler, quantile_transform\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\"\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rankmatch(from_mtx, to_mtx):\n",
    "\ttemp = np.sort(to_mtx.reshape((-1)))\n",
    "\ttemp2 = from_mtx.reshape((-1))\n",
    "\torder = np.argsort(temp2)\n",
    "\ttemp2[order] = temp\n",
    "\treturn temp2.reshape((len(from_mtx), -1))\n",
    "\n",
    "\n",
    "def create_mask(k=30, chrom=\"chr1\", origin_sparse=None):\n",
    "\tfinal = np.array(np.sum(origin_sparse, axis=0).todense())\n",
    "\tsize = origin_sparse[0].shape[-1]\n",
    "\ta = np.zeros((size, size))\n",
    "\tif k > 0:\n",
    "\t\tfor i in range(min(k, len(a))):\n",
    "\t\t\tfor j in range(len(a) - i):\n",
    "\t\t\t\ta[j, j + i] = 1\n",
    "\t\t\t\ta[j + i, j] = 1\n",
    "\t\ta = np.ones_like((a)) - a\n",
    "\t\n",
    "\tgap = np.sum(final, axis=-1, keepdims=False) == 0\n",
    "\tif cytoband_path is not None:\n",
    "\t\tgap_tab = pd.read_table(cytoband_path, sep=\"\\t\", header=None)\n",
    "\t\tgap_tab.columns = ['chrom', 'start', 'end', 'name', 'type']\n",
    "\t\t\n",
    "\t\tname = np.array(gap_tab['name'])\n",
    "\t\t# print (name)\n",
    "\t\tpqarm = np.array([str(s)[0] for s in name])\n",
    "\t\tgap_tab['pq_arm'] = pqarm\n",
    "\t\tgap_tab['length'] = gap_tab['end'] - gap_tab['start']\n",
    "\t\tsummarize = gap_tab.groupby(['chrom', 'pq_arm']).sum().reset_index()\n",
    "\t\t# print (summarize)\n",
    "\t\t\n",
    "\t\tif np.sum(summarize['pq_arm'] == 'p') > 0:\n",
    "\t\t\tsplit_point = \\\n",
    "\t\t\tnp.ceil(np.array(summarize[(summarize['chrom'] == chrom) & (summarize['pq_arm'] == 'p')]['length']) / res)[0]\n",
    "\t\telse:\n",
    "\t\t\tsplit_point = -1\n",
    "\t\t\n",
    "\t\tgap_list = gap_tab[(gap_tab[\"chrom\"] == chrom) & (gap_tab[\"type\"] == \"acen\")]\n",
    "\t\tstart = np.floor((np.array(gap_list['start'])) / res).astype('int')\n",
    "\t\tend = np.ceil((np.array(gap_list['end'])) / res).astype('int')\n",
    "\t\t\n",
    "\t\tfor s, e in zip(start, end):\n",
    "\t\t\ta[s:e, :] = 1\n",
    "\t\t\ta[:, s:e] = 1\n",
    "\telse:\n",
    "\t\tsplit_point = -1\n",
    "\ta[gap, :] = 1\n",
    "\ta[:, gap] = 1\n",
    "\t\n",
    "\treturn a, int(split_point)\n",
    "\n",
    "\n",
    "def process_one_chrom(chrom):\n",
    "\t# Get the raw sparse mtx list\n",
    "\tcell_type_info=pickle.load(open(os.path.join(data_dir, \"label_info.pickle\"), \"rb\"))\n",
    "\tcell_type=cell_type_info['cell_type']\n",
    "\torigin_sparse = np.load(os.path.join(raw_dir, \"%s_sparse_matrices.npy\" % chrom), allow_pickle=True)\n",
    "\tsize = origin_sparse[0].shape[0]\n",
    "\t# find centromere & gaps...\n",
    "\tmask, split_point = create_mask((int(1e5)), chrom, origin_sparse)\n",
    "\n",
    "\tbulk1 = np.array(np.sum(origin_sparse, axis=0).todense())\n",
    "\tprint(bulk1.shape)\n",
    "\tmask = (np.ones_like(bulk1) - mask)\n",
    "\tbulk1 *= mask\n",
    "\n",
    "\tif \"bulk_path\" in config:\n",
    "\t\timport cooler\n",
    "\t\tc = cooler.Cooler(\"%s::resolutions/%d\" % (config['bulk_path'], config['resolution']))\n",
    "\t\tbulk2 = np.array(c.matrix(sparse=False, balance=False).fetch(chrom)).astype('float')\n",
    "\t\tbulk2 *= mask\n",
    "\telse:\n",
    "\t\tbulk2 = None\n",
    "\n",
    "\tuse_rows_all = []\n",
    "\n",
    "\tif split_point >= 20 * 1000000 / res:\n",
    "\t\tslice_start_list, slice_end_list = [0, split_point], [split_point, len(bulk1)]\n",
    "\telse:\n",
    "\t\tslice_start_list, slice_end_list = [0], [len(bulk1)]\n",
    "\n",
    "\tbulk_compartment_all = []\n",
    "\treal_bulk_compartment_all = []\n",
    "\n",
    "\tbulk_model_list = []\n",
    "\tbulk_reverse_list = []\n",
    "\tbulk_slice_list = []\n",
    "\tuse_rows_list = []\n",
    "\t\n",
    "\t# temp_compartment_list_zscore = []\n",
    "\t# temp_compartment_list_quantile = []\n",
    "\n",
    "\tfor slice_start, slice_end in zip(slice_start_list, slice_end_list):\n",
    "\t\t\n",
    "\t\tbulk1_slice = bulk1[slice_start:slice_end, :]\n",
    "\t\tbulk1_slice = bulk1_slice[:, slice_start:slice_end]\n",
    "\t\tuse_rows = np.where(np.sum(bulk1_slice > 0, axis=-1) > 0.01 * len(bulk1_slice))[0]\n",
    "\t\tif len(use_rows) <= 1:\n",
    "\t\t\tprint(\"no reliable bins in slice:\", slice_start, slice_end)\n",
    "\t\t\tcontinue\n",
    "\t\tuse_rows_all.append(np.arange(slice_start, slice_end)[use_rows])\n",
    "\t\tuse_rows_list.append(use_rows)\n",
    "\t\tbulk1_slice = bulk1_slice[use_rows, :]\n",
    "\t\tbulk1_slice = bulk1_slice[:, use_rows]\n",
    "\t\t\n",
    "\t\tbulk_slice_list.append(bulk1_slice)\n",
    "\t\tbulk_expect = []\n",
    "\t\tfor k in range(len(bulk1_slice)):\n",
    "\t\t\tdiag = np.diag(bulk1_slice, k)\n",
    "\t\t\tbulk_expect.append(np.mean(diag))\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tbulk_compartment, model = test_compartment(bulk1_slice, return_PCA=True)\n",
    "\t\t\n",
    "\t\tif bulk2 is not None:\n",
    "\t\t\tbulk2_slice = bulk2[slice_start:slice_end, :]\n",
    "\t\t\tbulk2_slice = bulk2_slice[:, slice_start:slice_end]\n",
    "\t\t\tbulk2_slice = bulk2_slice[use_rows, :]\n",
    "\t\t\tbulk2_slice = bulk2_slice[:, use_rows]\n",
    "\t\t\treal_bulk_compartment, model = test_compartment(bulk2_slice, return_PCA=True)\n",
    "\t\telse:\n",
    "\t\t\treal_bulk_compartment = None\n",
    "\t\t\t\n",
    "\t\treverse_flag = False\n",
    "\t\tbulk_compartment_all.append(bulk_compartment)\n",
    "\t\tbulk_reverse_list.append(reverse_flag)\n",
    "\t\tbulk_model_list.append(model)\n",
    "\t\treal_bulk_compartment_all.append(real_bulk_compartment)\n",
    "\t\t\n",
    "\n",
    "\n",
    "\t\t\n",
    "\ttemp_compartment_list_all = [[] for i in range(len(use_rows_list))]\n",
    "\n",
    "\tcell_list = trange(len(origin_sparse))\n",
    "\t# print(cell_list)\n",
    "\ttemp = np.zeros((size, size))\n",
    "\tfor i in cell_list:\n",
    "\t\ttemp *= 0.0\n",
    "\t\tproba = np.array(origin_sparse[i].todense())\n",
    "\t\ttemp+= proba\n",
    "\t\ttemp = temp + temp.T\n",
    "\t\ttemp *= mask\n",
    "\t\t\n",
    "\t\tfor j in range(len(use_rows_list)):\n",
    "\t\t\tslice_start, slice_end = slice_start_list[j], slice_end_list[j]\n",
    "\t\t\ttemp_slice = temp[slice_start:slice_end, :]\n",
    "\t\t\ttemp_slice = temp_slice[:, slice_start:slice_end]\n",
    "\t\t\ttemp_select = temp_slice[use_rows_list[j], :]\n",
    "\t\t\ttemp_select = temp_select[:, use_rows_list[j]]\n",
    "\t\t\t# temp_select = rankmatch(temp_select, bulk_slice_list[j])\n",
    "\t\t\ttemp_compartment = test_compartment(temp_select, False, bulk_model_list[j], None)\n",
    "\t\t\tif bulk_reverse_list[j]:\n",
    "\t\t\t\ttemp_compartment = -1 * temp_compartment\n",
    "\t\t\t# temp_compartment_list_all[j].append(temp_compartment.reshape((-1)))\n",
    "\t\t\ttemp_compartment_list_all[j].append(temp_compartment)\n",
    "\t\t\t\n",
    "\tfor j in range(len(use_rows_list)):\n",
    "\t\ttemp_compartment_list_all[j] = np.stack(temp_compartment_list_all[j], axis=0)\n",
    "\t\t\n",
    "\n",
    "\treal_bulk_compartment = np.concatenate(real_bulk_compartment_all, axis=0) if bulk2 is not None else None\n",
    "\tbulk_compartment = np.concatenate(bulk_compartment_all, axis=0)\n",
    "\ttemp_compartment_list = np.concatenate(temp_compartment_list_all, axis=-1)\n",
    "\tuse_rows = np.concatenate(use_rows_all, axis=0)\n",
    "\tprint (chrom, \"finished\")\n",
    "\t# print(\"temp_compartment_list shape:\",temp_compartment_list.shape)\n",
    "\n",
    "\t##########从原始的接触图计算############################################\n",
    "\torigin_bulk_list_all=[[] for i in range(len(use_rows_list))]\n",
    "\tcell_type_info=pickle.load(open(os.path.join(data_dir, \"label_info.pickle\"), \"rb\"))\n",
    "\tcell_type=cell_type_info['cell_type']\n",
    "\t# sum_by_label=[]\n",
    "\ta={}\n",
    "\tb={}\n",
    "\tfor j in list(set(cell_type)):\n",
    "\t\t\n",
    "\t\tindices = [index for index, value in enumerate(cell_type) if value == j]\n",
    "\t\ta[j]=indices\n",
    "\t\tb[j] = np.zeros_like(np.array(origin_sparse[0]))\n",
    "\t\tfor i in a[j]:\n",
    "\t\t\t\tproba = np.array(origin_sparse[i])    \n",
    "\t\t\t\tb[j] +=proba\n",
    "\t\ttemp = np.array(b[j].item().todense())\n",
    "\t\t# print(temp.shape)\n",
    "\t\ttemp *= mask\n",
    "\t\tfor j in range(len(use_rows_list)):\n",
    "\t\t\tslice_start, slice_end = slice_start_list[j], slice_end_list[j]\n",
    "\t\t\ttemp_slice = temp[slice_start:slice_end, :]\n",
    "\t\t\ttemp_slice = temp_slice[:, slice_start:slice_end]\n",
    "\t\t\ttemp_select = temp_slice[use_rows_list[j], :]\n",
    "\t\t\ttemp_select = temp_select[:, use_rows_list[j]]\n",
    "\t\t\t# temp_select = rankmatch(temp_select, bulk_slice_list[j])\n",
    "\t\t\ttemp_compartment = test_compartment(temp_select, False, bulk_model_list[j], None)\n",
    "\t\t\tif bulk_reverse_list[j]:\n",
    "\t\t\t\ttemp_compartment = -1 * temp_compartment\n",
    "\t\t\t# temp_compartment_list_all[j].append(temp_compartment.reshape((-1)))\n",
    "\t\t\torigin_bulk_list_all[j].append(temp_compartment)\n",
    "        \n",
    "\tfor j in range(len(use_rows_list)):\n",
    "\t\torigin_bulk_list_all[j] = np.stack(origin_bulk_list_all[j], axis=0)\n",
    "\torigin_bulk_list = np.stack(origin_bulk_list_all[j], axis=-1)\n",
    "\n",
    "\treturn real_bulk_compartment, bulk_compartment, chrom, use_rows, size,origin_bulk_list,temp_compartment_list\n",
    "\n",
    "\t\n",
    "\t\t\t\n",
    "\n",
    "def process_calib_file(file_path):\n",
    "\ttab = pd.read_table(file_path , sep=\"\\t\", header=None)\n",
    "\ttab.columns = ['chrom', 'bin', 'value']\n",
    "\t# print(tab)\n",
    "\tchrom_start_end = np.load(os.path.join(temp_dir, \"chrom_start_end.npy\"))\n",
    "\ttab['chrom'] = np.array(tab['chrom']).astype('str')\n",
    "\tcalib_result = {}\n",
    "\tfor i, chrom in enumerate(chrom_list):\n",
    "\t\ttemp = tab[tab['chrom'] == chrom]\n",
    "\t\tsize = chrom_start_end[i, 1] - chrom_start_end[i, 0]\n",
    "\t\tvec = np.zeros(size)\n",
    "\t\tindice = (np.array(temp['bin'] / res)).astype('int')\n",
    "\t\tv = np.array(temp['value'])\n",
    "\t\tv[v == -1] = np.nan\n",
    "\t\tvec[indice] = v\n",
    "\t\tcalib_result[chrom] = vec\n",
    "\tnp.save(os.path.join(temp_dir, \"calib.npy\"), calib_result, allow_pickle=True)\n",
    "\n",
    "\n",
    "def start_call_compartment(output):\n",
    "\tp_list = []\n",
    "\tpool = ProcessPoolExecutor(max_workers=10)\n",
    "\t\n",
    "\tif \".hdf5\" not in output:\n",
    "\t\toutput += \".hdf5\"\n",
    "\twith h5py.File(os.path.join(temp_dir, output), \"w\") as output_f:\n",
    "\t\tresult = {}\n",
    "\t\tfor chrom in chrom_list:\n",
    "\t\t\t# real_bulk_compartment, bulk_compartment, temp_compartment_list, temp_compartment_zscore, temp_compartment_quantile, chrom, use_rows, size = process_one_chrom(chrom)\n",
    "\t\t\treal_bulk_compartment, bulk_compartment, chrom, use_rows, size,origin_bulk_list,temp_compartment_list = process_one_chrom(chrom)\n",
    "\t\t# \tp_list.append(pool.submit(process_one_chrom, chrom))\n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# for p in as_completed(p_list):\n",
    "\t\t# \treal_bulk_compartment, bulk_compartment, temp_compartment_list, temp_compartment_zscore, temp_compartment_quantile, chrom, use_rows, size = p.result()\n",
    "\t\t\tresult[chrom] = [real_bulk_compartment, bulk_compartment, use_rows, size,origin_bulk_list,temp_compartment_list]\n",
    "\t\t\t\n",
    "\t\tbin_chrom_list = []\n",
    "\t\tbin_start_list = []\n",
    "\t\tbin_end_list = []\n",
    "\t\tbulk_cp_all = []\n",
    "\t\treal_bulk_cp_all = []\n",
    "\t\tsc_cp_all = []\n",
    "\t\tsc_cp_raw = []\n",
    "\t\tsc_cp_zscore = []\n",
    "\t\tsc_cp_bulk = []\n",
    "\t\t#####################################整个数据集的bulk值##########################################################\n",
    "\t\t# grp = output_f.create_group('compartment')\n",
    "\t\t# bin = grp.create_group('bin')\n",
    "\t\t#############################################################################################################\n",
    "\t\t\n",
    "\t\tfor chrom in chrom_list:\n",
    "\t\t\treal_bulk_compartment, bulk_compartment, use_rows, size,origin_bulk_list ,temp_compartment_list= result[chrom]\n",
    "\t\t\t# print (use_rows)\n",
    "\t\t\t# print(chrom)\n",
    "\t\t\tlength = size\n",
    "\t\t\tbin_chrom_list += [chrom] * len(use_rows)\n",
    "\t\t\tbin_start_list.append((np.arange(length) * res).astype('int')[use_rows])\n",
    "\t\t\tbin_end_list.append(((np.arange(length) + 1) * res).astype('int')[use_rows])\n",
    "\t\t\tbulk_cp_all.append(bulk_compartment)\n",
    "\t\t\treal_bulk_cp_all.append(real_bulk_compartment)\n",
    "\t\t\t# sc_cp_all.append(temp_compartment_quantile)\n",
    "\t\t\t\n",
    "\t\t\t# print(\"temp_compartment_list shap to append:\",temp_compartment_list.shape)\n",
    "\t\t\tsc_cp_raw.append(temp_compartment_list)\n",
    "\t\t\t# print(\"sc_cp_raw shape\",(np.array(sc_cp_raw)).shape)\n",
    "\t\t\tsc_cp_bulk.append(origin_bulk_list)\n",
    "#####################################整个数据集的bulk值########################################################\t\t\t\n",
    "\t\t# bin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t#                    dtype=h5py.special_dtype(vlen=str))\n",
    "\t\t# bin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\t# bin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\t# # print(\"bulk_cp_all:\",np.array(bulk_cp_all).shape)\n",
    "\t\t# bulk_cp_all = np.concatenate(bulk_cp_all, axis=0)\n",
    "\t\t# # print(\"bulk_cp_all shape:\",np.array(bulk_cp_all).shape)\n",
    "\t\t# grp.create_dataset(\"bulk\", data=bulk_cp_all)\n",
    "\t\t\n",
    "\t\t# if real_bulk_compartment is not None:\n",
    "\t\t# \treal_bulk_cp_all = np.concatenate(real_bulk_cp_all, axis=0)\n",
    "\t\t# \tgrp.create_dataset(\"real_bulk\", data=real_bulk_cp_all)\n",
    "#############################################################################################################\t\t\n",
    "\t\t# sc_cp_all = np.concatenate(sc_cp_all, axis=-1)\n",
    "\t\tsc_cp_raw = np.concatenate(sc_cp_raw, axis=-2)\n",
    "\t\t# print('sc_cp_raw:',(np.array(sc_cp_raw)).shape)\n",
    "\t\t# sc_cp_zscore = np.concatenate(sc_cp_zscore, axis=-1)\n",
    "\t\tsc_cp_bulk=np.concatenate(sc_cp_bulk, axis=0)\n",
    "\t\t# print('sc_cp_bulk:',(np.array(sc_cp_bulk)).shape)\n",
    "\t\t\n",
    "\t\t# for cell in range(len(sc_cp_all)):\n",
    "\t\t# \tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_all[cell])\n",
    "\t\t\n",
    "\t\tgrp = output_f.create_group('compartment_raw')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tfor cell in range(len(sc_cp_raw)):\n",
    "\t\t\tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_raw[cell])\n",
    "\t\t\n",
    "\t\t# grp = output_f.create_group('compartment_zscore')\n",
    "\t\t# bin = grp.create_group('bin')\n",
    "\t\t# bin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t#                    dtype=h5py.special_dtype(vlen=str))\n",
    "\t\t# bin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\t# bin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\t# for cell in range(len(sc_cp_all)):\n",
    "\t\t# \tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_zscore[cell])\n",
    "\t\tgrp = output_f.create_group('compartment_bulk')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tcell_type_info=pickle.load(open(os.path.join(data_dir, \"label_info.pickle\"), \"rb\"))\n",
    "\t\tcell_type=cell_type_info['cell_type']\n",
    "\t\ti=0\n",
    "\t\tfor j in list(set(cell_type)):\n",
    "\t\t\tgrp.create_dataset(j, data=sc_cp_bulk[:,:,i])\n",
    "\t\t\ti+=1  \n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\toutput_f.close()\n",
    "\tpool.shutdown(wait=True)\n",
    "\n",
    "def test_compartment(matrix, return_PCA=False, model=None, expected = None):\n",
    "\tcontact = matrix\n",
    "\t# np.fill_diagonal(contact, np.max(contact))\n",
    "\t# contact = KRnormalize(matrix)\n",
    "\t# contact[np.isnan(contact)] = 0.0\n",
    "\tcontact = sqrt_norm(matrix)\n",
    "\tcontact = oe(contact, expected)\n",
    "\tnp.fill_diagonal(contact, 1)\n",
    "\twith warnings.catch_warnings():\n",
    "\t\twarnings.filterwarnings(\n",
    "\t\t\t\"ignore\", category=PearsonRConstantInputWarning\n",
    "\t\t)\n",
    "\t\tcontact = pearson(contact)\n",
    "\tnp.fill_diagonal(contact, 1)\n",
    "\tcontact[np.isnan(contact)] = 0.0\n",
    "\tif model is not None:\n",
    "\t\ty = model.transform(contact)\n",
    "\telse:\n",
    "\t\t# pca = PCA(n_components=3)       #保留的pca的列数\n",
    "\t\tpca = PCA(n_components=1)\n",
    "\t\ty = pca.fit_transform(contact)\n",
    "\tif return_PCA:\n",
    "\t\treturn y, pca\n",
    "\telse:\n",
    "\t\treturn y\n",
    "\n",
    "\n",
    "# args = parse_args()\n",
    "# print (args)\n",
    "# config = get_config('/home/python/higashi/cellcycle/1M/data/config_cellcycle.JSON')\n",
    "output = 'sc_and_bulk_cortex50k_hic'\n",
    "config=get_config('/home/python/higashi/dataset_hic/dataset2/cortex50k/config_cortex50k.json')\n",
    "res = config['resolution']\n",
    "data_dir = config['data_dir']\n",
    "temp_dir = config['temp_dir']\n",
    "raw_dir = os.path.join(temp_dir, \"raw\")\n",
    "if 'cytoband_path' in config:\n",
    "\tcytoband_path = config['cytoband_path']\n",
    "else:\n",
    "\tcytoband_path = None\n",
    "neighbor_num = config['neighbor_num']\n",
    "embedding_name = config['embedding_name']\n",
    "\n",
    "# if args.calib:\n",
    "# \tprocess_calib_file(args.calib_file)\n",
    "start_call_compartment(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higashi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
